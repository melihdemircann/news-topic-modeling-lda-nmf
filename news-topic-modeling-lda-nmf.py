# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KkTz2OPE4L-20jcTmxubaHQwwAvivEsd
"""

!pip install pyLDAvis -q

!pip install kagglehub -q

import numpy as np                   # Matematiksel hesaplamalar ve dizi iÅŸlemleri iÃ§in.
import pandas as pd                  # Verileri tablolar halinde yÃ¶netmek iÃ§in.
import matplotlib.pyplot as plt      # Grafik Ã§izdirmek iÃ§in temel kÃ¼tÃ¼phane.
import seaborn as sns                # Daha ÅŸÄ±k grafikler oluÅŸturmak iÃ§in.
from wordcloud import WordCloud      # Kelime bulutlarÄ±nÄ± oluÅŸturmak iÃ§in.
import re                            # Metin temizleme (link, sembol kaldÄ±rma) iÅŸlemleri iÃ§in.
import os                            # Dosya yolu ve sistem iÅŸlemleri iÃ§in.
from bs4 import BeautifulSoup        # HTML kodlarÄ±nÄ± metinden ayÄ±klamak iÃ§in.
from collections import Counter      # Kelime sayÄ±mlarÄ± yapmak iÃ§in.
import warnings                      # Gereksiz uyarÄ± mesajlarÄ±nÄ± gizlemek iÃ§in.


import nltk
from nltk import pos_tag, word_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer      # Kelime kÃ¶klerini bulmak iÃ§in.

# --- Makine Ã–ÄŸrenmesi ve Modelleme ---
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline        # Ä°ÅŸlem adÄ±mlarÄ±nÄ± birbirine baÄŸlar.
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # Metni sayÄ±ya dÃ¶ker.
from sklearn.decomposition import LatentDirichletAllocation, NMF             # Konu bulma algoritmalarÄ±.
from joblib import dump

# --- GÃ¶rselleÅŸtirme AraÃ§larÄ± ---
import pyLDAvis
import pyLDAvis.lda_model

# UyarÄ±larÄ± kapatÄ±yoruz
warnings.filterwarnings('ignore')




# Kelime kÃ¶klerini bulma ve dilbilgisi analizi iÃ§in gerekli paketler.
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')
nltk.download('omw-1.4')



#  Veri Setinin Ä°ndirilmesi
import kagglehub
path = kagglehub.dataset_download("gpreda/bbc-news")
print("Veri setinin indirildiÄŸi yol:", path)


# DosyalarÄ± listele
files = os.listdir(path)
print("KlasÃ¶rdeki dosyalar:", files)



# CSV dosyasÄ±nÄ± yÃ¼klÃ¼yoruz
df = pd.read_csv(f'{path}/bbc_news.csv')


df['Text'] = df['title'] + ' ' + df['description']



df = df.drop(columns=['title', 'pubDate', 'guid', 'link', 'description'])


# Yinelenen (kopya) verileri temizliyoruz
df = df.drop_duplicates()



#...





#  Metin Ã–n Ä°ÅŸleme FonksiyonlarÄ±
def get_wordnet_pos(word):
    """Kelimelerin dilbilgisi etiketini (isim, fiil vb.) WordNet formatÄ±na Ã§evirir."""
    tag = pos_tag([word])[0][1][0].upper()    #Kelimenin tÃ¼rÃ¼nÃ¼ al
    tag_dict = {
        'J': wordnet.ADJ,         # SÄ±fat
        'N': wordnet.NOUN,        # Ä°sim
        'V': wordnet.VERB,        # Fiil
        'R': wordnet.ADV          # Zarf
    }
    return tag_dict.get(tag, wordnet.NOUN)      # Bulunamazsa isim kabul et

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    """Metni temizler: HTML etiketlerini siler, linkleri kaldÄ±rÄ±r, kÃ¼Ã§Ã¼k harfe Ã§evirir ve kÃ¶klerine ayÄ±rÄ±r."""
    # HTML etiketlerini temizle
    soup = BeautifulSoup(text, 'lxml')
    text = soup.get_text(separator=' ')

    # URL'leri temizle
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)

    # Tokenization (Kelimelere ayÄ±rma)
    tokens = word_tokenize(text.lower())

    # Stopwords temizliÄŸi, sadece alfabetik karakterler ve Lemmatizasyon (KÃ¶k bulma)
    tokens = [
        lemmatizer.lemmatize(w, get_wordnet_pos(w))
        for w in tokens if w.isalpha() and w not in stop_words
    ]

    return ' '.join(tokens)

# Metin temizleme iÅŸlemini tÃ¼m satÄ±rlara uyguluyoruz
print("Metinler temizleniyor, lÃ¼tfen bekleyin...")
df['cleaned_text'] = df['Text'].apply(preprocess_text)


#......


#  Modelleme HattÄ± (Pipelines) OluÅŸturma
# LDA Modeli: Kelime frekanslarÄ±nÄ± (CountVectorizer) kullanÄ±r.
lda_pipeline = Pipeline([
    ('vectorizer', CountVectorizer(max_df=0.95, min_df=2)),
    ('lda', LatentDirichletAllocation(n_components=20, random_state=42))
])

# NMF Modeli: Kelime aÄŸÄ±rlÄ±klarÄ±nÄ± (TfidfVectorizer) kullanÄ±r.
nmf_pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer(max_df=0.95, min_df=2)),
    ('nmf', NMF(n_components=20, random_state=42))
])

# Modelleri eÄŸitiyoruz
print("Modeller eÄŸitiliyor...")
lda_pipeline.fit(df['cleaned_text'])
nmf_pipeline.fit(df['cleaned_text'])

# EÄŸitilmiÅŸ modelleri ve vektÃ¶rleÅŸtiricileri ayÄ±klÄ±yoruz
lda_model = lda_pipeline.named_steps['lda']
nmf_model = nmf_pipeline.named_steps['nmf']
count_vectorizer = lda_pipeline.named_steps['vectorizer']
tfidf_vectorizer = nmf_pipeline.named_steps['vectorizer']

count_data = count_vectorizer.transform(df['cleaned_text'])
tfidf_data = tfidf_vectorizer.transform(df['cleaned_text'])

#  KonularÄ± (Topics) GÃ¶rÃ¼ntÃ¼leme
def display_topics(model, feature_names, n_top_words=10):
    """Her konu iÃ§in en Ã¶nemli kelimeleri yazdÄ±rÄ±r."""
    for idx, topic in enumerate(model.components_):
        print(f'\nTopic {idx}: ' + ' | '.join(
            [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
        ))


print('\n--- LDA KonularÄ± ---')
display_topics(lda_model, count_vectorizer.get_feature_names_out())

print('\n--- NMF KonularÄ± ---')
display_topics(nmf_model, tfidf_vectorizer.get_feature_names_out())


##.....




# GÃ¶rselleÅŸtirme
# pyLDAvis ile interaktif LDA gÃ¶rselleÅŸtirmesi
pyLDAvis.enable_notebook()
lda_vis = pyLDAvis.lda_model.prepare(lda_model, count_data, count_vectorizer, n_jobs=1)


# Kelime BulutlarÄ± (WordClouds) oluÅŸturma ve kaydetme
print("Kelime bulutlarÄ± oluÅŸturuluyor...")
def create_wordclouds(model, vectorizer, prefix):
    for idx, topic in enumerate(model.components_):
        plt.figure(figsize=(8, 4))
        wordcloud = WordCloud(width=800, height=200, background_color='white') \
            .generate(' '.join([vectorizer.get_feature_names_out()[i]
                                for i in topic.argsort()[:-30 - 1:-1]]))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(f'{prefix} Topic {idx}')
        plt.savefig(f'{prefix.lower()}_word_cloud_topic_{idx}.png', dpi=300, bbox_inches='tight')
        plt.show()


create_wordclouds(lda_model, count_vectorizer, "LDA")
create_wordclouds(nmf_model,count_vectorizer, "NMF")

# BaskÄ±n konularÄ± ana veri Ã§erÃ§evesine ekleme
lda_doc_topics = lda_model.transform(count_data)
nmf_doc_topics = nmf_model.transform(tfidf_data)

df['LDA_Topic'] = lda_doc_topics.argmax(axis=1)
df['NMF_Topic'] = nmf_doc_topics.argmax(axis=1)

# TanÄ±mladÄ±ÄŸÄ±mÄ±z etiketleri sÃ¶zlÃ¼k olarak ekliyoruz
lda_topic_labels = {
    0: 'Sports â€“ International Competitions', 1: 'Royalty, Society & Public Figures',
    2: 'Sports â€“ Rugby & Controversy', 3: 'Science, History & Miscellaneous Reports',
    4: 'Human Interest & Entertainment', 5: 'Football â€“ Analysis & Club News',
    6: 'Covid & Health Updates', 7: 'Personal Stories & Human Interest',
    8: 'Energy, Economy & Global Affairs', 9: 'Accidents, Disasters & Health Issues',
    10: 'Sports & Business Personalities', 11: 'Tragedies & Maritime Incidents',
    12: 'UK Politics & Legal Matters', 13: 'Conflict & Refugee Crisis',
    14: 'Government Policy & Sanctions', 15: 'Entertainment & Cricket',
    16: 'Energy Prices & Covid Updates', 17: 'Culture & Identity',
    18: 'Protests & Civil Unrest', 19: 'Tennis, Celebrities & Social Issues'
}

nmf_topic_labels = {
    0: 'Covid, Science & Viral News', 1: 'Health, Sports & Environment',
    2: 'Conflict, Refugees & Global Affairs', 3: 'Terror & Human Movement',
    4: 'Health, Accidents & Human Interest', 5: 'Disasters, Politics & High-Profile Cases',
    6: 'Covid Updates & Sports', 7: 'Energy, Economy & Lifestyle',
    8: 'Sports, Accidents & Labor Issues', 9: 'Conflict, Humanitarian Crisis & Politics',
    10: 'Conflict, Sports & Human Interest', 11: 'Entertainment & International Affairs',
    12: 'Politics, Culture & Elections', 13: 'Covid Pandemic & Rules',
    14: 'Sports, Arts & International Competitions', 15: 'Covid, Royalty & Lifestyle',
    16: 'Politics, Sanctions & Refugee Policy', 17: 'History, Covid & Legal Issues',
    18: 'Social Media, Safety & Sports Tech', 19: 'Disasters, Entertainment & Personal Stories'
}

df['LDA Topic Label'] = df['LDA_Topic'].map(lda_topic_labels)
df['NMF Topic Label'] = df['NMF_Topic'].map(nmf_topic_labels)


##...

# 11. ADIM: Yeni Veri Ãœzerinde Test
new_text = ['''Rising natural gas prices have pushed up household energy bills, forcing families to adjust their monthly budgets. Economists warn that inflation may continue to climb, affecting spending habits and overall economic growth.''']

new_tfidf = tfidf_vectorizer.transform(new_text)
new_count = count_vectorizer.transform(new_text)

nmf_pred_topic = nmf_model.transform(new_tfidf).argmax()
lda_pred_topic = lda_model.transform(new_count).argmax()

print('\n--- Yeni Metin Tahmini ---')
print('Metin:', new_text[0][:100], "...")
print('NMF Tahmini:', nmf_topic_labels[nmf_pred_topic])
print('LDA Tahmini:', lda_topic_labels[lda_pred_topic])

#
print(" Model Tahmin EkranÄ± ")

def test_my_model():
    while True:
        print("\nLÃ¼tfen analiz edilmesini istediÄŸiniz haber metnini buraya yapÄ±ÅŸtÄ±rÄ±n:")
        print("(Ã‡Ä±kmak iÃ§in sadece 'q' yazÄ±p Enter'a basÄ±n)")

        user_text = input("> ")

        # Ã‡Ä±kÄ±ÅŸ kontrolÃ¼
        if user_text.lower() == 'q':
            print("Test iÅŸlemi bitti!")
            break

        if len(user_text.strip()) < 5:
            print("LÃ¼tfen daha uzun bir metin girin.")
            continue

        # 1. Metin Ã–n Ä°ÅŸleme
        cleaned_input = preprocess_text(user_text)

        # 2. VektÃ¶rleÅŸtirme (SayÄ±sal verilere dÃ¶nÃ¼ÅŸtÃ¼rme)
        input_count = count_vectorizer.transform([cleaned_input])
        input_tfidf = tfidf_vectorizer.transform([cleaned_input])

        # 3. Model Tahminleri
        # LDA Tahmini
        lda_probs = lda_model.transform(input_count)
        lda_topic_id = lda_probs.argmax()
        lda_confidence = lda_probs.max() * 100

        # NMF Tahmini
        nmf_probs = nmf_model.transform(input_tfidf)
        nmf_topic_id = nmf_probs.argmax()



        # 4. SonuÃ§larÄ± YazdÄ±rma
        print("\n" + "="*50)
        print(f"ğŸ“Œ GirdiÄŸiniz Metin Ã–zet: {user_text[:70]}...")
        print("-" * 50)

        # LDA Sonucu
        lda_label = lda_topic_labels.get(lda_topic_id, "TanÄ±mlanmamÄ±ÅŸ Konu")
        print(f" LDA Tahmini: Topic {lda_topic_id} -> **{lda_label}**")
        print(f" GÃ¼ven OranÄ±: %{lda_confidence:.2f}")

        print("-" * 50)

        # NMF Sonucu
        nmf_label = nmf_topic_labels.get(nmf_topic_id, "TanÄ±mlanmamÄ±ÅŸ Konu")
        print(f"ğŸ§¬ NMF Tahmini: Topic {nmf_topic_id} -> **{nmf_label}**")
        print("="*50 + "\n")

# Fonksiyonu Ã§alÄ±ÅŸtÄ±r
test_my_model()

# 12. ADIM: Modelleri Kaydetme
dump(lda_pipeline, 'lda_pipeline.joblib')
dump(nmf_pipeline, 'nmf_pipeline.joblib')
print("\n Modeller baÅŸarÄ±yla 'lda_pipeline.joblib' ve 'nmf_pipeline.joblib' adÄ±yla kaydedildi!")

# En son interaktif gÃ¶rseli gÃ¶ster
lda_vis

from google.colab import drive
drive.mount('/content/drive')
